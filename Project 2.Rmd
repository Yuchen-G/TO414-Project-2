---
title: "Project 2"
author: "Yuchen Gao, Bohan Guo, Zhiyang Liang, Frank Ren Qian, Zigui Song, Claris (Zihui) Wang"
date: "2/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
## Introduction
In this project, we are using the in vehicle coupon recommendation dataset from the UCI ML repository. The dataset is originally provided by the authors of the paper "A Bayesian Framework for Learning Rule Sets for Interpretable
Classification" (https://www.jmlr.org/papers/volume18/16-003/16-003.pdf). This dataset is the survey results that describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver.

### Business Insights
In this project, we are a market research firm conducting data analysis to help restaurants acquire more customers. Our clients (restaurants) include coffee houses, carryout & takeaway restaurants, bar, etc. Our clients would like to send coupons to the customers who have recorded their information in their databases. However, sending coupons to all of the subscribers will easily create list fatigue, which means the subscribers will stop engaging with email marketing campaigns. In order not to lose many subscribers while increase the coupon response rate, these restaurants would like us to find out who will most likely to accept the coupon given the information they provided in the database and other contextual information including weather, time, and teamperture. Thus, we would like to control for restuarant type, and use machine learning algorithms including linear, logistic, and decision tree models and stacking technique to create a model that can accurately identify who will accept the coupon given customer and contextual information.

#Data Preparation: train_test_split, (variable selection)
#Create 3 individual models: Decision Tree, SVM, Logistic Regression
#Create stacked model: Decision Tree or Linear Model


## Data Loading and Cleaning
```{r}

coupon_df = read.csv("in-vehicle-coupon-recommendation.csv")

# factorize columns
coupon_df$destination = factor(coupon_df$destination)
coupon_df$passanger = factor(coupon_df$passanger)
coupon_df$weather = factor(coupon_df$weather)
coupon_df$time = factor(coupon_df$time)
coupon_df$gender = factor(coupon_df$gender)
coupon_df$age = factor(coupon_df$age)
coupon_df$maritalStatus = factor(coupon_df$maritalStatus)
coupon_df$education = factor(coupon_df$education)
coupon_df$occupation = factor(coupon_df$occupation)
coupon_df$income = factor(coupon_df$income)
coupon_df$car = factor(coupon_df$car)
coupon_df$Bar = factor(coupon_df$Bar)
coupon_df$CoffeeHouse = factor(coupon_df$CoffeeHouse)
coupon_df$CarryAway = factor(coupon_df$CarryAway)
coupon_df$RestaurantLessThan20 = factor(coupon_df$RestaurantLessThan20)
coupon_df$Restaurant20To50 = factor(coupon_df$Restaurant20To50)
coupon_df$toCoupon_GEQ5min = NULL


# sum(is.na(coupon_df))
# 
# print(nrow(coupon_df), ncol(coupon_df))
# na.omit(coupon_df)
# print(nrow(coupon_df), ncol(coupon_df))

# summary(coupon_df)
bar_df = coupon_df[coupon_df["coupon"] == "Bar",]
carryout_df = coupon_df[coupon_df["coupon"] == "Carry out & Take away",]
cafe_df = coupon_df[coupon_df["coupon"] == "Coffee House",]
restaurant20To50_df = coupon_df[coupon_df["coupon"] == "Restaurant(20-50)",]
restaurantLessThan20_df = coupon_df[coupon_df["coupon"] == "Restaurant(<20)",]

bar_df["coupon"] = NULL
carryout_df["coupon"] = NULL
cafe_df["coupon"] = NULL
restaurant20To50_df["coupon"] = NULL
restaurantLessThan20_df["coupon"] = NULL

summary(bar_df)

```


Train test split and create training set for stacked model
```{r}

set.seed(42)

venue = restaurantLessThan20_df
venue["Y"] = as.factor(venue[,"Y"])

#If the number of the observation in the restaurant is even number, then evenly split the data into test and train. If the observation numbers are odd, then take one observation from the testing set and make them have equal number of observations.

test_set = sample(1:nrow(venue), 0.5*nrow(venue))

# even
df_train = venue[-test_set,]
df_test = venue[test_set,]

# odd
if (nrow(df_train)!=nrow(df_test)){
  df_train = df_train[2:nrow(df_train),]
}

stacked_train = as.data.frame(df_test["Y"])
colnames(stacked_train) = "gt"
```


```{r}
#Use lasso for variable selection
library(MASS)
library(glmnet)
library(caret)


X = model.matrix(Y ~ ., coupon_df)[, -1]
y = coupon_df$Y


grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 1, lambda = grid)
dim(coef(ridge.mod))

cv.out <- cv.glmnet(X, y, alpha = 1)

bestlam <- cv.out$lambda.1se
bestlam

out <- glmnet(X, y, alpha = 1)
coefs <- predict(out, type = "coefficients", s = bestlam)

selected_features = c(dimnames(coefs)[[1]][which(coefs != 0)])[2:56]
selected_features

```
By lasso selection, we figure out that some of the variables can be shrinken to zero and so that we can take it off from our model. For instance, the variables direction_opp, toCoupon_GEQ25min,toCoupon_GEQ15min,has_children. Hence, we can take off those 4 variables and focus on the rest of predictors.






```{r}
#Logistic Regression
logit.model = glm(Y ~.-direction_opp -toCoupon_GEQ25min - toCoupon_GEQ15min-has_children, data = df_train, family = "binomial")
summary(logit.model)

prob = predict(logit.model, df_train, type='response')
lr_pred = ifelse(prob>=0.5, 1,0)

confusionMatrix(factor(lr_pred), factor(df_train$Y))
```



```{r}
#Random Forest
library(caret)
library(randomForest)
# set.seed(42)
# 
# 
# ctrl = trainControl(method="cv", number=5, selectionFunction="best")
# grid <- expand.grid(.mtry=c(15,24))
# modellist <- list()
# 
# 
# for (ntree in c(50,100,300,500)) {
#   fit = train(Y ~., data = df_train, metric = "kappa", 
#   method = "rf", trControl = ctrl, ntree = ntree, tuneGrid = grid)
#   key <- toString(ntree)
#   modellist[[key]] <- fit
# }
# 
# results <- resamples(modellist)
# summary(results)

# ntree=300 has the highest Kappa and accuracy score. 
# Min.      1st Qu.   Median      Mean   3rd Qu.      Max.      NA's
# 0.5180287 0.5406788 0.5588248 0.5508877 0.5650060 0.5719001    0

```


```{r}
#tuned model
rf = randomForest(Y ~.-direction_opp -toCoupon_GEQ25min - toCoupon_GEQ15min-has_children, data = df_train, metric = "kappa", method = "rf", ntree = 100, mtry=21)

rf

rf_pred = predict(rf, df_test[colnames(df_test) != "Y"])

rf_cm = confusionMatrix(data = as.factor(rf_pred), reference = as.factor(df_test$Y))
rf_cm

importance(rf)
varImpPlot(rf)
```



```{r}
#SVM
library(kernlab)
svm = ksvm(Y ~.-direction_opp -toCoupon_GEQ25min - toCoupon_GEQ15min-has_children, data=df_train, kernel="vanilladot")
y_pred = predict(svm, df_test[colnames(df_test) != "Y"])

svm_cm = confusionMatrix(data = as.factor(rf_pred), reference = as.factor(df_test$Y))
svm_cm


```


```{r}
#Stacked Model
library(C50)
svm_column <- c(y_pred)
rf_column = c(rf_pred)
lr_column <- c(lr_pred)

stacked_set = data.frame(svm_column,rf_column,lr_column,df_test["Y"])


#stacked_train = as.data.frame(df_test["Y"])
#colnames(stacked_train) = "gt"
```


```{r}
test_set = sample(1:nrow(stacked_set), 0.2*nrow(stacked_set)) 

stack_train = stacked_set[-test_set,]
stack_test = stacked_set[test_set,]

stack_dt = C5.0(Y~., data = stack_train)

stack_dt
plot(stack_dt)

stack_pred <- predict(stack_dt, stack_test)
library(gmodels)

confusionMatrix(stack_pred, as.factor(stack_test$Y), mode="prec_recall")
```







