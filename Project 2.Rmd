---
title: "Project 2"
author: "Yuchen Gao, Bohan Guo, Zhiyang Liang, Frank Ren Qian, Zigui Song, Claris (Zihui) Wang"
date: "2/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction
> In this project, we are using the in vehicle coupon recommendation dataset from the UCI ML repository. The dataset is originally provided by the authors of the paper "A Bayesian Framework for Learning Rule Sets for Interpretable
Classification" (https://www.jmlr.org/papers/volume18/16-003/16-003.pdf). This dataset is the survey results that describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver.

### Business Insights
> We would like to control for coupon expiration time and coupon venue, which has 10 variations in total, and explore under which type of driving scenarios drivers would mostly likely to accept the coupon. Using machine learning algorithms including linear, logistic, and hierachical clustering models, we will be able to create a model that can accurately identify the driving scenarios that would render drivers most likely to accept the coupon. These models will be beneficial to advertising companies to make them better allocate their funding resources to target those specific driving scenarios. 

## Data Loading and Cleaning
```{r}
coupon_df = read.csv("in-vehicle-coupon-recommendation.csv")

# factorize columns
coupon_df$destination = factor(coupon_df$destination)
coupon_df$passanger = factor(coupon_df$passanger)
coupon_df$weather = factor(coupon_df$weather)
coupon_df$time = factor(coupon_df$time)
coupon_df$gender = factor(coupon_df$gender)
coupon_df$age = factor(coupon_df$age)
coupon_df$maritalStatus = factor(coupon_df$maritalStatus)
coupon_df$education = factor(coupon_df$education)
coupon_df$occupation = factor(coupon_df$occupation)
coupon_df$income = factor(coupon_df$income)
coupon_df$car = factor(coupon_df$car)
coupon_df$Bar = factor(coupon_df$Bar)
coupon_df$CoffeeHouse = factor(coupon_df$CoffeeHouse)
coupon_df$CarryAway = factor(coupon_df$CarryAway)
coupon_df$RestaurantLessThan20 = factor(coupon_df$RestaurantLessThan20)
coupon_df$Restaurant20To50 = factor(coupon_df$Restaurant20To50)

summary(coupon_df)

```

```{r}
logit.model = glm(Y ~., data = coupon_df, family = "binomial")
summary(logit.model)
# control for coupon expiration time (1d or 2hrs)
# coupon_df = split(coupon_df, coupon_df$expiration)
# coupon_df_1d = subset(coupon_df$"1d")
# coupon_df_2h = subset(coupon_df$"2h")

summary(coupon_df)
```

EDA: PCA to analyze the most important features, visualization
Create 3 individual models: Decision Tree, SVM, Logistic Regression
Create stacked model: Decision Tree or Random Forest

PCA
```{r}
library(faraway)

coupon_dummy = as.data.frame(model.matrix(~. -1, data = coupon_df))

coupon_pca = prcomp(coupon_dummy)

round(coupon_pca$sdev,3)

plot(1:10,coupon_pca$sdev[1:10],type = "l", xlab = "PC number", ylab = "SD of PC" )

plot(1:26,coupon_pca$sdev[1:26],type = "l", xlab = "PC number", ylab = "SD of PC" )
```



```{r}
#PLS
library(pls)
library(faraway)
plsg = plsr(Y~., data = coupon_df, ncomp = 26, validation = "CV", segments = 10)

pls_rmsCV = RMSEP(plsg, estimate = "CV")
plot(pls_rmsCV$val, xlab = "Pc number", ylab = "CV RMS")
which.min(pls_rmsCV$val)

```


```{r}
#Use lasso for variable selection
library(MASS)
library(glmnet)


X = model.matrix(Y ~ ., coupon_df)[, -1]
y = coupon_df$Y


grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 1, lambda = grid)
dim(coef(ridge.mod))

cv.out <- cv.glmnet(X, y, alpha = 1)

bestlam <- cv.out$lambda.1se
bestlam

out <- glmnet(X, y, alpha = 1)
coefs <- predict(out, type = "coefficients", s = bestlam)

selected_features = c(dimnames(coefs)[[1]][which(coefs != 0)])[2:60]
selected_features

```
By lasso selection, we figure out that some of the variables can be shrinken to zero and so that we can take it off from our model. For instance, the variables direction_opp, toCoupon_GEQ25min,toCoupon_GEQ5min,has_children. Hence, we can take off those 4 variables and focus on the rest of predictors.



```{r}
library(randomForest)

n = sqrt(ncol(coupon_df))

n

rm = randomForest(Y ~ . -toCoupon_GEQ5min -toCoupon_GEQ25min - has_children - direction_opp , data = coupon_df, mtry = n,importance=TRUE)


```




