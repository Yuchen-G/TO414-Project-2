---
title: "Project 2"
author: "Yuchen Gao, Bohan Guo, Zhiyang Liang, Frank Ren Qian, Zigui Song, Claris (Zihui) Wang"
date: "2/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction
> In this project, we are using the in vehicle coupon recommendation dataset from the UCI ML repository. The dataset is originally provided by the authors of the paper "A Bayesian Framework for Learning Rule Sets for Interpretable
Classification" (https://www.jmlr.org/papers/volume18/16-003/16-003.pdf). This dataset is the survey results that describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver.

### Business Insights
> In this project, we are a market research firm conducting data analysis to help restaurants acquire more customers. Our clients (restaurants) include coffee houses, carryout & takeaway restaurants, bar, etc. Our clients would like to send coupons to the customers who have recorded their information in their databases. However, sending coupons to all of the subscribers will easily create list fatigue, which means the subscribers will stop engaging with email marketing campaigns. In order not to lose many subscribers while increase the coupon response rate, these restaurants would like us to find out who will most likely to accept the coupon given the information they provided in the database and other contextual information including weather, time, and teamperture. Thus, we would like to control for restuarant type, and use machine learning algorithms including linear, logistic, and decision tree models and stacking technique to create a model that can accurately identify who will accept the coupon given customer and contextual information.

Data Preparation: train_test_split, (variable selection)
Create 3 individual models: Decision Tree, SVM, Logistic Regression
Create stacked model: Decision Tree or Linear Model


## Data Loading and Cleaning
```{r}
coupon_df = read.csv("in-vehicle-coupon-recommendation.csv")

# factorize columns
coupon_df$destination = factor(coupon_df$destination)
coupon_df$passanger = factor(coupon_df$passanger)
coupon_df$weather = factor(coupon_df$weather)
coupon_df$time = factor(coupon_df$time)
coupon_df$gender = factor(coupon_df$gender)
coupon_df$age = factor(coupon_df$age)
coupon_df$maritalStatus = factor(coupon_df$maritalStatus)
coupon_df$education = factor(coupon_df$education)
coupon_df$occupation = factor(coupon_df$occupation)
coupon_df$income = factor(coupon_df$income)
coupon_df$car = factor(coupon_df$car)
coupon_df$Bar = factor(coupon_df$Bar)
coupon_df$CoffeeHouse = factor(coupon_df$CoffeeHouse)
coupon_df$CarryAway = factor(coupon_df$CarryAway)
coupon_df$RestaurantLessThan20 = factor(coupon_df$RestaurantLessThan20)
coupon_df$Restaurant20To50 = factor(coupon_df$Restaurant20To50)

# summary(coupon_df)
bar_df = coupon_df[coupon_df["coupon"] == "Bar",]
carryout_df = coupon_df[coupon_df["coupon"] == "Carry out & Take away",]
cafe_df = coupon_df[coupon_df["coupon"] == "Coffee House",]
restaurant20To50_df = coupon_df[coupon_df["coupon"] == "Restaurant(20-50)",]
restaurantLessThan20_df = coupon_df[coupon_df["coupon"] == "Restaurant(<20)",]
```

Train test split and create training set for stacked model
```{r}
set.seed(42)
test_set = sample(1:nrow(coupon_df), 0.2*nrow(coupon_df))

df_train = df[-test_set,]
df_test = df[test_set,]

stacked_train = 
```


Feature Selection
```{r}
# coupon.dummy = as.data.frame(model.matrix(~. -1, data=coupon_df))

# logit.model = glm(Y ~., data = coupon_df, family = "binomial")
# summary(logit.model)
```


Logistic Regression
```{r}
venue = cafe_df
logit.model = glm(Y ~., data = venue, family = "binomial")
summary(logit.model)
```


Random Forest
```{r}
<<<<<<< HEAD
rfurl = "https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-14.tar.gz"

library(randomForest)
rf = randomForest(Y ~ ., data="coupon_df")
=======
library(faraway)

coupon_dummy = as.data.frame(model.matrix(~. -1, data = coupon_df))

coupon_pca = prcomp(coupon_dummy)

round(coupon_pca$sdev,3)

plot(1:10,coupon_pca$sdev[1:10],type = "l", xlab = "PC number", ylab = "SD of PC" )

plot(1:26,coupon_pca$sdev[1:26],type = "l", xlab = "PC number", ylab = "SD of PC" )
```



```{r}
#PLS
library(pls)
library(faraway)
plsg = plsr(Y~., data = coupon_df, ncomp = 26, validation = "CV", segments = 10)

pls_rmsCV = RMSEP(plsg, estimate = "CV")
plot(pls_rmsCV$val, xlab = "Pc number", ylab = "CV RMS")
which.min(pls_rmsCV$val)
>>>>>>> 4f986b66201fbe88797f00bd068ff51d459677ac

```


```{r}
#Use lasso for variable selection
library(MASS)
library(glmnet)


X = model.matrix(Y ~ ., coupon_df)[, -1]
y = coupon_df$Y


grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 1, lambda = grid)
dim(coef(ridge.mod))

cv.out <- cv.glmnet(X, y, alpha = 1)

bestlam <- cv.out$lambda.1se
bestlam

out <- glmnet(X, y, alpha = 1)
coefs <- predict(out, type = "coefficients", s = bestlam)

selected_features = c(dimnames(coefs)[[1]][which(coefs != 0)])[2:60]
selected_features

```
By lasso selection, we figure out that some of the variables can be shrinken to zero and so that we can take it off from our model. For instance, the variables direction_opp, toCoupon_GEQ25min,toCoupon_GEQ5min,has_children. Hence, we can take off those 4 variables and focus on the rest of predictors.



```{r}
library(randomForest)

n = sqrt(ncol(coupon_df))

n

rm = randomForest(Y ~ . -toCoupon_GEQ5min -toCoupon_GEQ25min - has_children - direction_opp , data = coupon_df, mtry = n,importance=TRUE)


```





